<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Plamen Kolev's Blog</title>
        <link rel="stylesheet" media="screen" href="/assets/styles/asciidoctor-17b5ecd0da3e95005da9b5aab4b3e6dedd50479c6d8798ae1acbda75335c8ca4.css" /> 

    
    <link href="https://fonts.googleapis.com/css?family=Montserrat|Nunito" rel="stylesheet"/>
    <meta name="viewport" content="initial-scale=1" maximum-scale="4"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-b4efd2171ee890e5dd7d1621f5f267318f46d46c368105bb1fb92bc0466a4e92.png" />
    <meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="xv+cbtUaT4fZm/ZyFH2MecVTyRPGTdIq9T33eC6Owju+Y6/qvX+HzMlXn3GgZMZrRgM4kU1ygIJZemAwNB7F5g==" />
    <link rel="stylesheet" media="screen" href="/assets/bootstrap.min-7307797e236f9b1392c0a52e1aa8603d4bf231f0518fa9cc68799d7b9a9e8804.css" />
    <link rel="stylesheet" media="all" href="/assets/application-c3494be4828045db77302b630748a887df3cd1702fa8637bbea8e2393092cf57.css" data-turbolinks-track="reload" />
        <link rel="stylesheet" media="screen" href="/assets/styles/github-gist.min-acfbf141dac7c1dea84e2deb4fda718cb49b73a2610335f12c6f1202257066a1.css" /> 

  </head>

  <body>
    <div class="wrapper">
    <div class="col-md-12 col-sm-12 col-xs-12 navbar-container">
        <div class="col-md-2 col-sm-3 col-xs-3">
            <div class="logo_menu menu_circle">
                <a class="navbar-brand nopadding logo_menu" href="/"><img alt="Website logo image in the menu" src="/assets/logo-161a20f47c35dad89bdb75c577d99d5f9bee4b4b39b80134193829702d20c24b.svg" /></a>            </div>
        </div>
        <div class="col-md-10 col-sm-9 col-xs-9 pull-right align-right">
            <ul class="nav navbar-nav main_menu capital">
                <li class="menu_item">
                    <a href="/">home</a>
                    <span class="divider">&frasl;</span>
                </li>
                <li class="menu_item">
                    <a href="/articles.html">Articles</a>
                    <span class="divider">&frasl;</span>
                </li>
                <li class="menu_item">
                    <a href="/biography.html">Biography</a>
                </li>
            </ul>
        </div>
    </div>
    
</div>
    <div class="col-md-12 col-sm-12 col-xs-12 nopadding nomargin main_content" role="main">
      
<div class="col-md-12 nopadding_left_rigt">
    <div class="wrapper">
        <div class="col-md-12 nopadding">

               <div class="article_thumbnail">
                    <div class="article_wrapper">
                    <h1 class="main_head block">      Data classification using Neural Networks and Genetic Programming
</h1>
                    <p class="clarification article_date">01 July 2018</p>

                        <div>
                            <p class="article_container"><p>Plamen Kolev</p>

<h1 id="genetic-programming">Genetic Programming</h1>

<h2 id="revised-justification">Revised justification</h2>

<p><img src="/media/gpimages/gp.png" alt="Image taken from
http://www.ra.cs.uni-tuebingen.de/software/JCell/images/docbook/gp.png" />
As the first biologically inspired machine learning algorithm, I decided
to use Genetic Programming. Genetic Programming is suitable for
classifying multiple classes. An Oxford journal about classifying
microarray datasets talks about ways GP can be used to classify and
diagnose particular types of cancer (Kun-Hong L. et al. 2008). An
implementation of a Genetic Programming for classification works by
applying divide and conquer strategies to the training data, splitting
it into multiple sets of decision trees. The leaves of the trees
represent different classes (Jeroen Eggermont et al. 2004).</p>

<p>In the original proposal, I make the case for using Genetic Algorithm
with k-nearest neighbours, but later decided to not use it, as GP
frameworks such as <strong>epochx</strong> contain all the necessary libraries and
tuning strategies that will provide a better result with the ability to
do more fine-grain tuning. It will also allow me to explore a more
generic solution.</p>

<p>In genetic programming, each decision branch is formed by a mathematical
expression (functions). For the dataset, I experimented with different
‘syntax’ functions. The findings are documented in the tuning section
for this algorithm, but the most effective were the trigonometric ones.</p>

<h2 id="description-of-implementation">Description of implementation</h2>

<p>As mentioned above, I used the <strong>epochx</strong> library framework to bootstrap
my implementation. I created a new source package called <strong>GP</strong>, which
contains <code>GPClassifier.java</code>, <code>GPControl.java</code> and <code>GPWrapper.java</code>. The
implementation of the <code>GPControl</code> class is very similar to the provided
sample one, as its purpose is to test the actual implementation. Minor
modifications were made to accommodate for the newly created data
structures. Another modification is exposing the <code>generateSubSolution</code>
to return <code>GPClassifier</code>, as this class is used in the main method to
write performance and configuration statistics to a report file called
<code>gpstats.csv</code>.</p>

<p>The <code>GPClassifier</code> class contains a genetic program instance that has
learned on the training data and implements <code>classifyInstance</code> and
<code>printClassifier</code>. Those methods are passed to the underlying genetic
programming class <code>GPWrapper</code>, the class is acting as a middle man
between the control and the genetic programming class.</p>

<p>The <code>GPWrapper</code> class contains all the necessary constructs and tunable
variables for the <strong>epoch</strong> framework,which are described in the tuning
section.</p>

<p><strong>GPWrapper.java constructor.</strong></p>

<p>``` java
syntax.add(new SignumFunction());
syntax.add(new SineFunction());
syntax.add(new CosineFunction());
syntax.add(new ArcTangentFunction());
syntax.add(new AbsoluteFunction());</p>

<p>syntax.add(new AddFunction());
syntax.add(new DivisionProtectedFunction());
syntax.add(new SubtractFunction());
…
for (int i = 0; i &lt; Attributes.getNumAttributes(); i++) {
    syntax.add( variables[i] = new Variable(“dim” + i, Double.class) );
}
```</p>

<p>The constructor allows for an arbitrary attributes to be passed as the
syntax, but the syntax functions are specially tuned for the given
dataset, as trigonometric functions are suitable for classifying the
circular representation of the data.</p>

<p>An auxiliary function called <code>parseData</code> is provided just to store the
data in an internal array for later usage.<br />
This class also returns itself as an instance when <code>generateClassifier</code>
is called. The method is responsible for setting the different tuning
parameters and training the genetic program.</p>

<p><strong>GPWrapper.java classifyInstance method.</strong></p>

<p><code>java
public int classifyInstance(Instance ins){
    GPCandidateProgram a = (GPCandidateProgram) this.getProgramSelector().getProgram();
    for (int i = 0; i &lt; Attributes.getNumAttributes(); i++) {
        this.variables[i].setValue(ins.getRealAttribute(i));
    }
    Double result = (Double) a.evaluate();
    return  (int) Math.round(result);
}
</code></p>

<p>The <code>classifyInstance</code> method is used after the GP is trained to
determine the class of the data (black or white).It uses the
<code>GPCandidateProgram</code> object with the best fitness (closest to 0). That
program is given the instance attributes and evaluates a solution. The
output is an integer that represents the class.</p>

<p>As the <code>GPWrapper</code> extends from the epochx’s <code>GPModel</code>, it also
implements <code>getfitness</code> method. This method is applied to each member of
the population (<strong>candidate program</strong>). Each candidate produces an
output when given the input data points from the training set. If the
output of the program matches the class (predicts it), a score variable
is incremented. The best candidate program should have a score which
should be as close to the size of the training set as possible.</p>

<p><code>printClassifier</code> simply finds the fittest program from the candidate
pool and prints its representation, which is just a string of nested
functions.</p>

<p>Similar to the <code>NeuralClassifier</code>, this class also contains <code>setconfig</code>
function, which allows for environmental variables to be set and used to
tune different parameters. Finally, the classifier also implements
visualise, which draws a representation of the training and test data to
the screen using <code>Jpanel</code> in a class called <code>Picasso</code>.</p>

<h2 id="tuning-for-the-provided-dataset">Tuning for the provided dataset</h2>

<p>For the tuning, I exposed a variety of parameters that are used when
creating and training the classifier. The most important tunable
parameter is the population size, which represents the number of
generated random program candidates. A large population range between
600 - 1200 members provided enough diversity to generate the most
efficient classifiers and below 600, the accuracy relied mainly on the
random seed value and luck, making the predictions unreliable.</p>

<p>I also decided to experiment with the generation size. When a large
generation set is configured, at some point the population diversity
evens out and the fittest individual does not change much. A value
between 20-70 generations is the most appropriate, as it gives the
programs enough time to evolve and mutate, but keeps the running time
short.</p>

<p>Another important factor in the efficiency was selecting the syntax
functions when creating the program trees. They had very significant
impact on the evaluation time and efficiency.The performance of each
function varied, as some were very beneficial, some had not much impact
and others had a negative effect . The implementation does not include
flags for tuning which type of function to use, but rather includes a
selection of the the most effective ones. The highest result I was able
to achieve is using <code>SignumFunction</code> as well as the trigonometry
functions. I included division, subtraction and addition, as they helped
with the program’s diversity.</p>

<p>Experimenting with number of selected elite members, crossover, mutation
and reproduction probability did not provide a significant difference
during my experiments.</p>

<h2 id="performance-report">Performance report</h2>

<table>
  <thead>
    <tr>
      <th>Runtime</th>
      <th>Efficiency in %</th>
      <th>Population size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.506</td>
      <td>48.421052631578945</td>
      <td>10</td>
    </tr>
    <tr>
      <td>7.528</td>
      <td>86.31578947368422</td>
      <td>40</td>
    </tr>
    <tr>
      <td>10.396</td>
      <td>88.42105263157895</td>
      <td>80</td>
    </tr>
    <tr>
      <td>30.763</td>
      <td>90.52631578947368</td>
      <td>200</td>
    </tr>
    <tr>
      <td>53.695</td>
      <td>86.31578947368422</td>
      <td>500</td>
    </tr>
    <tr>
      <td>163.121</td>
      <td>90.52631578947368</td>
      <td>1200</td>
    </tr>
    <tr>
      <td>232.42</td>
      <td>92.63157894736842</td>
      <td>1600</td>
    </tr>
  </tbody>
</table>

<p>GP algorithm with different population size</p>

<p><img src="/media/gpimages/gp_population.png" alt="gp population" /></p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Generation number</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.087</td>
      <td>48.91304347826087</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1.267</td>
      <td>54.736842105263165</td>
      <td>4</td>
    </tr>
    <tr>
      <td>2.045</td>
      <td>75.26315789473685</td>
      <td>8</td>
    </tr>
    <tr>
      <td>3.582</td>
      <td>86.8421052631579</td>
      <td>18</td>
    </tr>
    <tr>
      <td>12.429</td>
      <td>91.05263157894737</td>
      <td>20</td>
    </tr>
    <tr>
      <td>10.704</td>
      <td>87.89473684210526</td>
      <td>24</td>
    </tr>
    <tr>
      <td>17.317</td>
      <td>90.52631578947368</td>
      <td>28</td>
    </tr>
    <tr>
      <td>16.921</td>
      <td>84.73684210526315</td>
      <td>30</td>
    </tr>
    <tr>
      <td>14.252</td>
      <td>93.15789473684211</td>
      <td>40</td>
    </tr>
    <tr>
      <td>25.816</td>
      <td>90.0</td>
      <td>60</td>
    </tr>
  </tbody>
</table>

<p>Table showing runs with different generations (stopping criteria)</p>

<p><img src="/media/gpimages/gp_generations.png" alt="gp generations" /></p>

<p>I also experimented with different crossover probability, number of
elite chromosomes and mutation but they did not improve or lower the
efficiency. The difference I noticed is the runtime, as these parameters
allow the GP to perform additional operations pore often. I have
included a table with my findings for completeness.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Time</td>
      <td>Efficiency %</td>
      <td>Elite population count</td>
    </tr>
    <tr>
      <td>22.11</td>
      <td>92.63157894736842</td>
      <td>1</td>
    </tr>
    <tr>
      <td>38.558</td>
      <td>93.15789473684211</td>
      <td>4</td>
    </tr>
    <tr>
      <td>42.268</td>
      <td>90.0</td>
      <td>12</td>
    </tr>
    <tr>
      <td>34.73</td>
      <td>97.89473684210527</td>
      <td>18</td>
    </tr>
    <tr>
      <td>43.304</td>
      <td>88.94736842105263</td>
      <td>24</td>
    </tr>
    <tr>
      <td>35.337</td>
      <td>96.3157894736842</td>
      <td>30</td>
    </tr>
  </tbody>
</table>

<p>Table of different size of the elite population</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Crossover probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>77.397</td>
      <td>90.0</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td>87.546</td>
      <td>94.21052631578948</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>82.201</td>
      <td>87.36842105263159</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>58.289</td>
      <td>93.15789473684211</td>
      <td>0.8</td>
    </tr>
    <tr>
      <td>62.03</td>
      <td>94.21052631578948</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>

<p>Table of different crossover probability</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Mutation probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>15.015</td>
      <td>85.78947368421052</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td>94.777</td>
      <td>89.47368421052632</td>
      <td>0.2</td>
    </tr>
    <tr>
      <td>79.374</td>
      <td>88.94736842105263</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>105.695</td>
      <td>93.6842105263158</td>
      <td>0.6</td>
    </tr>
    <tr>
      <td>62.391</td>
      <td>88.42105263157895</td>
      <td>0.8</td>
    </tr>
    <tr>
      <td>71.862</td>
      <td>91.57894736842105</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>

<p>Mutation probability</p>

<h1 id="neural-network">Neural Network</h1>

<h2 id="revised-justification-1">Revised Justification</h2>

<p><img src="/media/neuralimages/network.jpg" alt="Taken from
https://www.codeproject.com/KB/dotnet/predictor/network.jpg" /></p>

<p>As mentioned in the coursework proposal, I chose Neural network as the
method for classification for several reasons: It is suitable for
classifying multiple ‘features’ as neural networks have been widely used
for speech recognition, facial recognition and malignant cancer
detection (Álvarez L. et al. 2009). The Universal approximation theorem
also states that a neural network has the ability to can compute every
function. Yoshua Bengio and Yann LeCun also mention in their paper that
neural networks do not have limitations in the efficiency of the
representation of certain types of functions as compared to other
approaches.</p>

<p>I also wanted to use a neural network, as it is a very exciting and hot
topic in computing and wanted to experiment with the different
activation functions and parameters.</p>

<h2 id="implementation-description">Implementation description</h2>

<p>Implementing the solution was a multi-part process. During my research,
I found it suitable to use Neuroph java framework for the supervised
neural network. I then outlined key features and functions that need to
be implemented, tuned and integrated. The following is a detailed
description of them.</p>

<p>Firstly, I created a special package called Neural, which contains the
main class <code>Control.java</code> of the application and the
<code>NeuralClassifier.java</code> class witch is a wrapper for Neuroph. For the
code of <code>Control.java</code>, I used most of the sample code provided (time
keeping, printing training and test data, etc.) with some minor
modifications. I also replaced the Wrapper classifier with the neural
implementation file <code>NeuralClassifier.java</code>. The new main class was also
modified to allow for stats dumps to csv file used to generate
statistics for the report.</p>

<p>My <code>NeuralClassifier.java</code> extends from the <code>Classifier</code> class and thus
implements <code>classifyInstance</code> and <code>printClassifier</code>. Its purpose is to
be a middle man between the given framework and Neuroph. Because Neuroph
uses its own data representation for the training and testing sets, an
auxiliary function is provided as part of the implementation called
<code>instanceSetToDataSet</code> which parses the InstanceSet and converts it to
DataSet used by Neuroph.</p>

<p><strong>NeuralClassifier.java function instanceSetToDataSet.</strong></p>

<p><code>java
public DataSet instanceSetToDataSet(InstanceSet trainingData){
        int dataRows = Attributes.getNumAttributes();
        DataSet td = new DataSet(dataRows, 1);
        ...
            // Data normalisation code skipped
            // Data scaling code skipped
            td.addRow(new DataSetRow(inputs, new double[]{instance.getClassValue()}));
        }
        ...
    }
</code></p>

<p>Here, the TrainingSet is converted to Data set with dynamic rows. This
function is used in the constructor to bootstrap the
<code>MultiLayerPerceptron</code>.</p>

<p>Constructing the actual class is done in the following way:</p>

<p><strong>NeuralClassifier.java function constructor.</strong></p>

<p><code>java
NeuralClassifier(InstanceSet trainingSet){
    ...
    this.initConfig();
    DataSet trainingData = this.instanceSetToDataSet(trainingSet);
    this.mlp = new MultiLayerPerceptron(ACTIVATION_FUNCTION,
              Attributes.getNumAttributes(), HIDDENNODES, 1);
    BackPropagation b = (BackPropagation) mlp.getLearningRule();
    b.setMaxIterations(ITERATIONS);
    b.setMaxError(MAXERROR);
    b.setLearningRate(LEARNINGRATE);
    ...
    this.mlp.learn(trainingData);
}
</code></p>

<p>As mentioned in the proposal, I am using Multilayer perceptron with
back-propagation, the above function creates and configures the neural
network instance. As described in the lecture notes, the
<code>MultilayerPerceptron</code> uses an activation function which is one of the
parameters for tuning, I have set the <code>Sigmoid Transfer function</code> as the
default as the highest results are often achieved using it, but others
are explored as part of the performance report. For the input layer, I
use as many neurons as data attributes in the training set, allowing for
a generic solution when applying different datasets.</p>

<p>Neuroph has a construct that allows for the tuning of extra parameters,
like the number of iterations, max errors and learning rate, they are
also configured and demonstrated in the performance section. The given
<code>Attributes</code>, <code>InstanceSet</code> and <code>Instance</code> classes were used throughout,
as they provided helpful methods give the necessary information to make
the solution generic. When going though the source code, I also noticed
that some examples were scaling and normalising the data, so I decided
to include these features into the implementation. I used
<code>DecimalScaleNormalizer.java</code> function <code>normalizeScale</code> example from the
source directory of neuroph to optionally scale down each data point by
a factor of 10 based on a flag. This function and the flag <code>normalise</code>
in <code>instanceSetToDataSet</code> function are extra tunable parameters that I
experimented with but found to cause harm to both the performance and
the efficiency.</p>

<p><strong>NeuralClassifier.java function classifyInstance.</strong></p>

<p>``` java
public int classifyInstance(Instance ins) {
    // provide the classifier with the inputs
    int numattrs = Attributes.getNumAttributes();
    double[] attributes = new double[numattrs];
    for (int i = 0; i &lt; numattrs; i++) {
        attributes[i] = ins.getRealAttribute(i);
    }
    this.mlp.setInput(attributes);</p>

<pre><code>// calculate neural network output
this.mlp.calculate();
double[] result = this.mlp.getOutput();
int prediction = (int) Math.round(result[0]);
if(prediction &gt; Attributes.getNumAttributes() || prediction &lt; 0){
    return -1;
}
return prediction; } ```
</code></pre>

<p><code>classifyInstance</code> function is used to return a prediction after the
network has gone through the learning process. In this case, the
<code>MultiLayerPerceptron</code> is given the instance data , after which the
probability for each class is computed and returned. The prediction is 0
for black, 1 for white and -1 if it goes outside the domain range, or
more generally, between 0 to n-1 where n is the class range.</p>

<p><strong>NeuralClassifier.java function printClassifier.</strong></p>

<p><code>java
public void printClassifier() {
    for (Double weight : mlp.getWeights()) {
        System.out.print("Output weights: " + weight);
    }
    System.out.println();
}
</code></p>

<p>For printing the final classifier, I grab the weight of each node in the
neural network and print it out.</p>

<p>Finally, I created a helper function called <code>initConfig()</code> to read
environment variables that allow for dynamic tuning and shell execution
of the solution for generating statistics. I have included the shell
scripts used for generating the performance data under “gp_shell” for
the genetic programming algorithm and “neural_shell” for the neural
network algorithm. They have been developed and tested under Ubuntu
16.04 and work by using environmental variables. When running the
program, it will create a report file in the current directory with the
performance, duration and the tuned parameters.</p>

<p>Finally, the classifier also implements visualise, which draws a
representation of the training and test data to the screen using
<code>Jpanel</code> in a class called <code>Picasso</code>.<br />
<em>#</em> Description of adjustment and tuning<br />
A neural network allows for a variety of parameter tweaking and
different optimisations.<br />
Reading up on different approaches for training a neural network, The
report (Moriera, M 1995) describes the purpose of a learning rate. The
paper suggests to use a learning rate that is sufficiently large to
allow for the learning process but small enough to guarantee
effectiveness. For that purpose, I used the learning rule function
<code>setLearningRate(value)</code> and found out that a very small value in the
range of <code>0.01-0.1</code> usually yields the best results. I experimented with
more than one hidden layers, but often the result was identical or
slightly worse but it increased the learning time, sometimes by a factor
of two. Hence I decided to ignore the tuning of multiple layers, and
instead allow for the tuning of different hidden nodes in the single
layer. Just one layer is used, because the data to classify is not
complex enough to benefit from it (not many features). Tuning the
learning rate had a significant impact on the accuracy and I found that
a lower learning late had a very positive impact on the correctness of
the classifier for a very small performance cost.</p>

<p>Neuroph also provides couple of activation functions as part of the
<code>MultilayerPerceptron</code> package. Experimenting with them, I thought
initially that the gaussian function will yield the best result, but by
far the most efficient function was <code>sigmoid</code>, which consistently
achieves results above 90% accuracy. To contrast that with the <strong>tanH</strong>
function, it has an extremely negative effect on the accuracy, making
the prediction absolutely wrong. Another function that performed very
poorly is the <strong>linear</strong> function, which yields 50% accuracy, making the
classifier more similar to a coin toss.</p>

<p>An article on standardising data for neural networks (McCaffrey J. 2014)
an argument for normalisation of the data is made. I have allowed for a
normalisation flag in my implementation, but enabling it has a negative
effect due to the data being evenly distributed and relatively short. As
mentioned above, data can also be scaled, but again, this also has a
negative effect on the accuracy.</p>

<h2 id="performance-report-1">Performance report</h2>

<table>
  <thead>
    <tr>
      <th><strong>Time in seconds</strong></th>
      <th><strong>Efficiency in %</strong></th>
      <th>Number of iterations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.189</td>
      <td>85.26315789473684</td>
      <td>1</td>
    </tr>
    <tr>
      <td>0.489</td>
      <td>86.8421052631579</td>
      <td>20</td>
    </tr>
    <tr>
      <td>0.838</td>
      <td>87.36842105263159</td>
      <td>40</td>
    </tr>
    <tr>
      <td>1.121</td>
      <td>87.89473684210526</td>
      <td>60</td>
    </tr>
    <tr>
      <td>1.371</td>
      <td>88.42105263157895</td>
      <td>80</td>
    </tr>
    <tr>
      <td>1.642</td>
      <td>88.94736842105263</td>
      <td>100</td>
    </tr>
    <tr>
      <td>10.2</td>
      <td>96.3157894736842</td>
      <td>748</td>
    </tr>
    <tr>
      <td>13.524</td>
      <td>96.3157894736842</td>
      <td>997</td>
    </tr>
    <tr>
      <td>26.391</td>
      <td>97.36842105263158</td>
      <td>1993</td>
    </tr>
    <tr>
      <td>28.076</td>
      <td>97.36842105263158</td>
      <td>2242</td>
    </tr>
    <tr>
      <td>21.338</td>
      <td>96.84210526315789</td>
      <td>2989</td>
    </tr>
    <tr>
      <td>32.731</td>
      <td>96.84210526315789</td>
      <td>3238</td>
    </tr>
  </tbody>
</table>

<p>Efficiency with different iterations</p>

<p>The table above shows improvement in performance with the increase of
iterations. In my case, it peaked at about 1500-2000 iterations, at
which point it did not have a significant impact on the efficiency but
it impacted the running time.</p>

<p><img src="/media/neuralimages/iterations.png" alt="iterations" /></p>

<table>
  <thead>
    <tr>
      <th><strong>Time</strong></th>
      <th><strong>Accuracy in %</strong></th>
      <th><strong>Hidden nodes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6.379</td>
      <td>82.10526315789474</td>
      <td>1</td>
    </tr>
    <tr>
      <td>9.108</td>
      <td>84.21052631578947</td>
      <td>3</td>
    </tr>
    <tr>
      <td>12.434</td>
      <td>91.05263157894737</td>
      <td>6</td>
    </tr>
    <tr>
      <td>12.125</td>
      <td>94.21052631578948</td>
      <td>10</td>
    </tr>
    <tr>
      <td>14.309</td>
      <td>98.42105263157895</td>
      <td>17</td>
    </tr>
    <tr>
      <td>16.069</td>
      <td>97.36842105263158</td>
      <td>23</td>
    </tr>
    <tr>
      <td>15.644</td>
      <td>98.42105263157895</td>
      <td>24</td>
    </tr>
    <tr>
      <td>17.264</td>
      <td>95.78947368421052</td>
      <td>29</td>
    </tr>
    <tr>
      <td>17.871</td>
      <td>97.89473684210527</td>
      <td>33</td>
    </tr>
    <tr>
      <td>18.304</td>
      <td>97.36842105263158</td>
      <td>36</td>
    </tr>
    <tr>
      <td>20.96</td>
      <td>97.36842105263158</td>
      <td>40</td>
    </tr>
  </tbody>
</table>

<p>Impact on different hidden nodes</p>

<p><img src="/media/neuralimages/hidden_nodes.png" alt="hidden nodes" /></p>

<p>In this case, increasing the amount of hidden nodes had a significant
impact on the accuracy, with it peaking at about 17-24 nodes. I found
that using 18 hidden nodes fairly reliably produced around 98% accuracy,
thus chose it as the base for some runs.</p>

<p>To keep this report short, I will include a graph of learning rate and
max error results without including the tables, but the data can be
found and viewed under neuralstats.csv</p>

<p><img src="/media/neuralimages/learningrate.png" alt="learningrate" /></p>

<p><img src="/media/neuralimages/maxewrrorrate.png" alt="maxewrrorrate" /></p>

<p>I observed that learning rate and max error rate were the most efficient
at around 1% and increasing them had mostly negative effect on the
effectiveness.</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Normalised</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>15.487,</td>
      <td>82.63157894736842</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>15.009</td>
      <td>98.42105263157895</td>
      <td>No</td>
    </tr>
  </tbody>
</table>

<p>Normalisation vs. no normalisation</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Scaled down</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>15.652,</td>
      <td>74.21052631578947</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td>15.705,</td>
      <td>96.3157894736842</td>
      <td>No</td>
    </tr>
  </tbody>
</table>

<p>Down scaling (factor of 10) vs. no down scaling</p>

<p>As mentioned above, experimenting with scaling or normalising the data
did not yield a positive effect, on the contrary, it reduced the
classifier’s ability to make accurate predictions.</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Efficiency in %</th>
      <th>Activation function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>16.638</td>
      <td>95.78947368421052</td>
      <td>SIGMOID</td>
    </tr>
    <tr>
      <td>16.37</td>
      <td>92.63157894736842</td>
      <td>GAUSSIAN</td>
    </tr>
    <tr>
      <td>14.283</td>
      <td>50.0</td>
      <td>LINEAR</td>
    </tr>
    <tr>
      <td>13.596</td>
      <td>17.20430107526882</td>
      <td>TANH</td>
    </tr>
    <tr>
      <td>14.101,</td>
      <td>50.0</td>
      <td>TRAPEZOID</td>
    </tr>
  </tbody>
</table>

<p>Different activation functions</p>

<h1 id="comparison-between-methods">Comparison between methods</h1>

<p>The difference between genetic programming and neural networks (MLP) was
very noticeable. The neural network produced a more generic solution, as
less knowledge was provided to it. To contrast that with the GP, many
variables had to be tuned specifically to achieve the optimal result
using syntax functions that are relevant to the solution. Neural
networks are also faster and more efficient, outperforming the GP
implementation. The implementation of the neural network is also a bit
harder to reason with and the library hides most of the complexity away
from the developer.</p>

<p>On the other hand, the Genetic Programming algorithm seemed more simple
and intuitive, the tunable parameters had a direct impact on the result
and the classification representation is simple. The program is also
less generic as it required special tuning to achieve good results. It
also does not provide consistent accuracy, where as the Neural Network
had much smaller margin of error.</p>

<h1 id="general-note">General note</h1>

<p>The sourcecode folder contains gp_shell and neural_shell bash scripts
that will work only on linux.</p>

<p>To manually run the java executable, use the following commands<br />
<code>java -cp CSC3423.jar Biocomputing.Neural.Control dataset/Training.arff
dataset/Test.arff</code><br />
<code>java -cp CSC3423.jar Biocomputing.GP.GPControl dataset/Training.arff
dataset/Test.arff</code><br />
Where <code>CSC3423.jar</code> is the executable jar file in the out folder and the
arff files are the Training and Testing data sets.<br />
The Neural network accepts the following environmental variables:
<code>ITERATIONS</code>, <code>GENERATIONS</code>,<code>ACTIVATION_FUNCTION</code>,
<code>HIDDENNODES</code>,<code>MAXERROR</code>,<code>LEARNINGRATE</code>,<code>NORMALISE</code>,<code>SCALE</code>.<br />
GPClassifier environmental varialbes: <code>POPULATION_SIZE</code>, <code>GENERATIONS</code>,
<code>ELITES</code>, <code>CROSSOVER_PROB</code>, <code>MUTATION_PROB</code>, <code>REPRODUCTION_PROB</code> and
<code>TOURNAMENT_SIZE</code>.<br />
In windows, one can use the command <code>set HIDDENNODES=5</code> followed by the
java run command to set these values.</p>

<h1 id="references">References</h1>

<ol>
  <li>
    <p>Moreira, M. (1995). Neural Networks with Adaptive Learning Rate and
Momentum Terms. Available at
<a href="https://infoscience.epfl.ch/record/82307/files/95-04.pdf">https://infoscience.epfl.ch/record/82307/files/95-04.pdf</a>. Accessed
on 12.12.2016</p>
  </li>
  <li>
    <p>Kun-Hong L. et al. (2008). A genetic programming-based approach to
the classification of multiclass microarray datasets. Available at
<a href="http://bioinformatics.oxfordjournals.org/content/25/3/331.full">http://bioinformatics.oxfordjournals.org/content/25/3/331.full</a>.
Accessed on 14.12.2016</p>
  </li>
  <li>
    <p>McCaffrey J. (2014). How To Standardize Data for Neural Networks.
Available at
<a href="https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx">https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx</a>.
Accessed on 14.12.2016.</p>
  </li>
  <li>
    <p>Jeroen Eggermont et al (2004). Genetic Programming for Data
Classification: Partitioning the Search Space. Available at
<a href="http://liacs.leidenuniv.nl/~kosterswa/SAC2003final.pdf">http://liacs.leidenuniv.nl/~kosterswa/SAC2003final.pdf</a>. Accessed
on 15.12.2016</p>
  </li>
  <li>
    <p>Álvarez L. et al. (2009). Artificial neural networks applied to
cancer detection in a breast screening programme. Available at
<a href="http://ac.els-cdn.com/S0895717710001378/1-s2.0-S0895717710001378-main.pdf?_tid=c4a6e3c0-c2b3-11e6-9ddd-00000aab0f6b&amp;acdnat=1481798983_5428dbaa26e2d27c669d7906a5c1a77a">http://ac.els-cdn.com/S0895717710001378/1-s2.0-S0895717710001378-main.pdf?_tid=c4a6e3c0-c2b3-11e6-9ddd-00000aab0f6b&amp;acdnat=1481798983_5428dbaa26e2d27c669d7906a5c1a77a</a>.
Accessed on 15.12.2016</p>
  </li>
  <li>
    <p>Bengio Y., LeCun Y. (2007) Scaling Learning Algorithms towards AI.
Available at
<a href="http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf">http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf</a>
(Accessed: 16 Nov 2016).</p>
  </li>
</ol>

<h1 id="credits">Credits</h1>

<ol>
  <li>
    <p>Genetic programming tree image taken from
<a href="http://www.ra.cs.uni-tuebingen.de/software/JCell/images/docbook/gp.png">http://www.ra.cs.uni-tuebingen.de/software/JCell/images/docbook/gp.png</a></p>
  </li>
  <li>
    <p>Multilayer perceptron with back-propagation image taken from
<a href="https://www.codeproject.com/KB/dotnet/predictor/network.jpg">https://www.codeproject.com/KB/dotnet/predictor/network.jpg</a></p>
  </li>
</ol>
</p>
                        </div>
                    </div>
                    <p class="article_tag_container">
                            <span class="article_tag">Neural networks</span>
                            <span class="article_tag"> Genetic Programming</span>
                            <span class="article_tag"> MLP</span>
                            <span class="article_tag"> Machine Learning</span>
                            <span class="article_tag"> epochx</span>
                            <span class="article_tag"> classification</span>
                            <span class="article_tag"> sigmoidal</span>
                    </p>
                </div>

        </div>

    </div>
</div>






    </div>
    <div role="contentinfo" class="col-md-12 col-sm-12 col-xs-12 navbar-fixed-bottom footer nopadding nomargin">  
      <div class="footer_part1"></div>
<div class="footer_part2">
    <div class="wrapper">
        <div class="col-md-12">
            <div class="col-md-6 inline-block pull-left">
                <table>
                    <tr>
                        <th><strong>Contact</strong></th>
                        <th></th>
                    </tr>

                    <tr>
                        <td>Email:</td>
                        <td><a href="mailto:p.kolev22@gmail.com">p.kolev22@gmail.com</a></td>
                    </tr>
                    <tr>
                        <td>LinkedIn:</td>
                        <td><a href="https://www.linkedin.com/in/plamen-kolev">https://www.linkedin.com/in/plamen-kolev</a></td>
                    </tr>
                    <tr>
                        <td>Github:</td>
                        <td><a href="https://github.com/plamen-kolev">https://github.com/plamen-kolev</a></td>
                    </tr>
                </table>
            </div>
            <div class="col-md-6 inline-block align-right" id="mobile_footer_right">
                <p>
                    Plamen Kolev <span class="white">2016-2020</span><br/>
                    All rights reserved
                </p>
            </div>
        </div>
    </div>
</div>
    
  </body>

  <script src="/assets/application-349f5f5e3c9554ec0a886e7e8b4c2aa8457f60f1dfff3f213e1439658b82088d.js" data-turbolinks-track="reload"></script>
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

</html>
